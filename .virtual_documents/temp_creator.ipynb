import os
import requests
import zstandard as zstd
import chess.pgn
import io
from tqdm.notebook import tqdm
import pandas as pd


# Point JAVA_HOME to your Java 17
os.environ["JAVA_HOME"] = "/opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home"

# Update PATH so 'java' command uses Java 17
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

# Check
get_ipython().getoutput("java -version")

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, BooleanType

spark = SparkSession.builder \
    .appName("LichessPGN") \
    .config("spark.driver.memory", "16g") \
    .config("spark.executor.memory", "16g") \
    .getOrCreate()



import requests
import zstandard as zstd
import chess.pgn
import io
from tqdm import tqdm

url = "https://database.lichess.org/standard/lichess_db_standard_rated_2025-10.pgn.zst"
parquet_dir = "data/01_Raw/lichess_1M_games"

BATCH_SIZE = 10000
MAX_GAMES = 1_000_000

batch = []
count = 0
file_index = 0

def extract_moves(game):
    moves = []
    node = game
    while node.variations:
        node = node.variations[0]
        moves.append(node.move.uci())
    return moves

def game_has_evals(game):
    node = game
    while node.variations:
        node = node.variations[0]
        if node.comment and "%eval" in node.comment:
            return True
    return False

with requests.get(url, stream=True) as r:
    r.raise_for_status()
    dctx = zstd.ZstdDecompressor()

    with dctx.stream_reader(r.raw) as reader:
        text_stream = io.TextIOWrapper(reader, encoding="utf-8")
        games_iter = iter(lambda: chess.pgn.read_game(text_stream), None)

        for game in tqdm(games_iter, desc="Parsing games"):
            if game is None:
                break
            if count >= MAX_GAMES:
                break

            headers = game.headers
            batch.append({
                "white_rating": int(headers.get("WhiteElo", 0)),
                "black_rating": int(headers.get("BlackElo", 0)),
                "time_control": headers.get("TimeControl"),
                "opening": headers.get("Opening"),
                "result": headers.get("Result"),
                "moves": extract_moves(game),
                "move_count": len(extract_moves(game)),
                "has_evals": game_has_evals(game)
            })

            count += 1

            # ---------- write each batch to parquet ----------
            if len(batch) >= BATCH_SIZE:
                sdf_batch = spark.createDataFrame(batch)
                sdf_batch.write.mode("append").parquet(parquet_dir)
                batch = []   # clear memory

# Write remaining leftovers
if batch:
    sdf_batch = spark.createDataFrame(batch)
    sdf_batch.write.mode("append").parquet(parquet_dir)

print("Done. Saved:", count, "games.")



sdf = spark.read.parquet("data/01_Raw/lichess_1M_games")
print(sdf.count())
sdf.show(5)



