import os
import requests
import zstandard as zstd
import chess.pgn
import io
from tqdm.notebook import tqdm
import pandas as pd


# Point JAVA_HOME to your Java 17
os.environ["JAVA_HOME"] = "/opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home"

# Update PATH so 'java' command uses Java 17
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

# Check
get_ipython().getoutput("java -version")

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, BooleanType

spark = SparkSession.builder \
    .appName("LichessPGN") \
    .config("spark.driver.memory", "16g") \
    .config("spark.executor.memory", "16g") \
    .getOrCreate()



url = "https://database.lichess.org/standard/lichess_db_standard_rated_2025-10.pgn.zst"

temp_dir = "data/01_Raw"
parquet_file = os.path.join(temp_dir, "lichess_1M_games.parquet")

# ------------------------
# 3️⃣ Helper functions
# ------------------------
def extract_moves(game):
    moves = []
    node = game
    while node.variations:
        node = node.variations[0]
        moves.append(node.move.uci())
    return moves

def game_has_evals(game):
    node = game
    while node.variations:
        node = node.variations[0]
        if node.comment and "%eval" in node.comment:
            return True
    return False

# ------------------------
# 4️⃣ Streaming and parsing
# ------------------------
rows = []

with requests.get(url, stream=True) as r:
    r.raise_for_status()
    dctx = zstd.ZstdDecompressor()
    with dctx.stream_reader(r.raw) as reader:
        text_stream = io.TextIOWrapper(reader, encoding="utf-8")

        games_iter = iter(lambda: chess.pgn.read_game(text_stream), None)

        for i, game in enumerate(tqdm(games_iter, desc="Parsing games", unit="games")):
            if game is None:
                break
            if i >= 1000000:  # limit to 1 million games
                break

            headers = game.headers
            moves = extract_moves(game)

            row = {
                "white_rating": int(headers.get("WhiteElo", 0)),
                "black_rating": int(headers.get("BlackElo", 0)),
                "time_control": headers.get("TimeControl"),
                "opening": headers.get("Opening"),
                "result": headers.get("Result"),
                "moves": moves,
                "move_count": len(moves),
                "has_evals": game_has_evals(game)
            }

            rows.append(row)

# ------------------------
# 5️⃣ Save as Parquet
# ------------------------
sdf = pd.DataFrame(rows)
sdf.to_parquet(parquet_file, engine='pyarrow', index=False)

print(f"Saved {len(sdf)} games to {parquet_file}")


parquet_file = "data/01_Raw/lichess_1M_games.parquet"

# Check if the file exists
if os.path.exists(parquet_file):
    sdf = pd.read_parquet(parquet_file, engine='pyarrow')
    print(f"Loaded {len(sdf)} games")
    print(sdf.head())  # display first few rows
else:
    print(f"File not found: {parquet_file}")



